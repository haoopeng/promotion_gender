{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/shared/0/datasets/mag/Promotion/'\n",
    "mag_dir = '/shared/0/datasets/mag/raw_data/'\n",
    "\n",
    "def fpath(filename):\n",
    "    return os.path.join(data_root, filename)\n",
    "\n",
    "def yield_one_line(filename, delimiter=',', quoting = csv.QUOTE_ALL):\n",
    "    '''a generator which produce one line of a given file'''\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file, delimiter=delimiter, quoting=quoting)\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            count += 1\n",
    "            if count % 10000000 == 0:\n",
    "                print('processed %d lines...' % (count))\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_root+'alt_dois.pickle', 'rb') as ofile:\n",
    "    dois = pickle.load(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1218710"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_to_doi = {}\n",
    "doi_to_pid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000000 lines...\n",
      "processed 20000000 lines...\n",
      "processed 30000000 lines...\n",
      "processed 40000000 lines...\n",
      "processed 50000000 lines...\n",
      "processed 60000000 lines...\n",
      "processed 70000000 lines...\n",
      "processed 80000000 lines...\n",
      "processed 90000000 lines...\n",
      "processed 100000000 lines...\n",
      "processed 110000000 lines...\n",
      "processed 120000000 lines...\n",
      "processed 130000000 lines...\n",
      "processed 140000000 lines...\n",
      "processed 150000000 lines...\n",
      "processed 160000000 lines...\n",
      "processed 170000000 lines...\n",
      "processed 180000000 lines...\n",
      "processed 190000000 lines...\n",
      "processed 200000000 lines...\n",
      "processed 210000000 lines...\n",
      "processed 220000000 lines...\n"
     ]
    }
   ],
   "source": [
    "for line in yield_one_line(mag_dir+'Papers.csv', delimiter=',', quoting=csv.QUOTE_ALL):\n",
    "    pid, doi = line[0], line[2]\n",
    "    # lowercase all DOIs\n",
    "    doi = doi.lower()\n",
    "    if doi in dois:\n",
    "        pid_to_doi[pid] = doi\n",
    "        doi_to_pid[doi] = pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAG `pid` and `doi` is not uniquely matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150268"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pid_to_doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1134395"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doi_to_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10.13039/501100003329', 1238),\n",
       " ('10.13039/501100007273', 190),\n",
       " ('10.4000/terrain.16638', 67),\n",
       " ('10.1126/science.361.6406.988-c', 62),\n",
       " ('10.13039/501100003176', 48),\n",
       " ('10.1002/ajpa.23489', 35),\n",
       " ('10.1002/pds.4629', 32),\n",
       " ('10.4172/0976-4860-c1-002', 30),\n",
       " ('10.21767/2573-4482-c1-002', 29),\n",
       " ('10.4172/2573-0347-c9-041', 28)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter(pid_to_doi.values()).items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dois_valid = set()\n",
    "\n",
    "for doi, cn in Counter(pid_to_doi.values()).items():\n",
    "    if cn == 1:\n",
    "        dois_valid.add(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123240"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dois_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in list(pid_to_doi.keys()):\n",
    "    doi = pid_to_doi[pid]\n",
    "    if doi not in dois_valid:\n",
    "        del pid_to_doi[pid]\n",
    "        \n",
    "for doi in list(doi_to_pid.keys()):\n",
    "    if doi not in dois_valid:\n",
    "        del doi_to_pid[doi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123240"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pid_to_doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123240"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doi_to_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9216630699674245"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pid_to_doi) / len(dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dois_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dois_authors_mag = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000000 lines...\n",
      "processed 20000000 lines...\n",
      "processed 30000000 lines...\n",
      "processed 40000000 lines...\n",
      "processed 50000000 lines...\n",
      "processed 60000000 lines...\n",
      "processed 70000000 lines...\n",
      "processed 80000000 lines...\n",
      "processed 90000000 lines...\n",
      "processed 100000000 lines...\n",
      "processed 110000000 lines...\n",
      "processed 120000000 lines...\n",
      "processed 130000000 lines...\n",
      "processed 140000000 lines...\n",
      "processed 150000000 lines...\n",
      "processed 160000000 lines...\n",
      "processed 170000000 lines...\n",
      "processed 180000000 lines...\n",
      "processed 190000000 lines...\n",
      "processed 200000000 lines...\n",
      "processed 210000000 lines...\n",
      "processed 220000000 lines...\n",
      "processed 230000000 lines...\n",
      "processed 240000000 lines...\n",
      "processed 250000000 lines...\n",
      "processed 260000000 lines...\n",
      "processed 270000000 lines...\n",
      "processed 280000000 lines...\n",
      "processed 290000000 lines...\n",
      "processed 300000000 lines...\n",
      "processed 310000000 lines...\n",
      "processed 320000000 lines...\n",
      "processed 330000000 lines...\n",
      "processed 340000000 lines...\n",
      "processed 350000000 lines...\n",
      "processed 360000000 lines...\n",
      "processed 370000000 lines...\n",
      "processed 380000000 lines...\n",
      "processed 390000000 lines...\n",
      "processed 400000000 lines...\n",
      "processed 410000000 lines...\n",
      "processed 420000000 lines...\n",
      "processed 430000000 lines...\n",
      "processed 440000000 lines...\n",
      "processed 450000000 lines...\n",
      "processed 460000000 lines...\n",
      "processed 470000000 lines...\n",
      "processed 480000000 lines...\n",
      "processed 490000000 lines...\n",
      "processed 500000000 lines...\n",
      "processed 510000000 lines...\n",
      "processed 520000000 lines...\n",
      "processed 530000000 lines...\n",
      "processed 540000000 lines...\n",
      "processed 550000000 lines...\n",
      "processed 560000000 lines...\n",
      "processed 570000000 lines...\n",
      "processed 580000000 lines...\n",
      "processed 590000000 lines...\n"
     ]
    }
   ],
   "source": [
    "for line in yield_one_line(mag_dir+'PaperAuthorAffiliations.txt', delimiter='\\t', quoting=csv.QUOTE_NONE):\n",
    "    pid, aid, aff_id, seq, name = line[:5]\n",
    "    if pid in pid_to_doi:\n",
    "        doi = pid_to_doi[pid]\n",
    "        dois_authors_mag[doi].append((aid, aff_id, seq, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123240"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dois_authors_mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dois_authors_mag) / len(doi_to_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fpath('dois_authors_mag.json'), 'w') as ofile:\n",
    "    for doi in dois_authors_mag:\n",
    "        pid = doi_to_pid[doi]\n",
    "        authors = dois_authors_mag[doi]\n",
    "        row = {'doi': doi, 'mag_pid': pid, 'authors': authors}\n",
    "        ofile.write(json.dumps(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dois_authors_mag = {}\n",
    "\n",
    "with open(fpath('dois_authors_mag.json'), 'r') as ofile:\n",
    "    for row in ofile:\n",
    "        row = json.loads(row)\n",
    "        doi, authors = row['doi'], row['authors']\n",
    "        dois_authors_mag[doi] = authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aids = set()\n",
    "for doi in dois_authors_mag:\n",
    "    authors = dois_authors_mag[doi]\n",
    "    for aid, aff_id, seq, name in authors:\n",
    "#         seq = int(seq)\n",
    "#         if seq == 1:\n",
    "        all_aids.add(aid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3335676"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_aids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000000 lines...\n",
      "processed 20000000 lines...\n",
      "processed 30000000 lines...\n",
      "processed 40000000 lines...\n",
      "processed 50000000 lines...\n",
      "processed 60000000 lines...\n",
      "processed 70000000 lines...\n",
      "processed 80000000 lines...\n",
      "processed 90000000 lines...\n",
      "processed 100000000 lines...\n",
      "processed 110000000 lines...\n",
      "processed 120000000 lines...\n",
      "processed 130000000 lines...\n",
      "processed 140000000 lines...\n",
      "processed 150000000 lines...\n",
      "processed 160000000 lines...\n",
      "processed 170000000 lines...\n",
      "processed 180000000 lines...\n",
      "processed 190000000 lines...\n",
      "processed 200000000 lines...\n",
      "processed 210000000 lines...\n",
      "processed 220000000 lines...\n",
      "processed 230000000 lines...\n",
      "processed 240000000 lines...\n"
     ]
    }
   ],
   "source": [
    "aids_metric = dict()\n",
    "\n",
    "for line in yield_one_line(mag_dir+'Authors.txt', delimiter='\\t', quoting=csv.QUOTE_NONE):\n",
    "    aid, rank, num_paper, num_cite = line[0], line[1], line[5], line[6]\n",
    "    if aid in all_aids:\n",
    "        aids_metric[aid] = (rank, num_paper, num_cite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3335676"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aids_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fpath('aids_metric_mag.json'), 'w') as ofile:\n",
    "    for aid, info in aids_metric.items():\n",
    "        row = {'aid': aid, 'metric': info}\n",
    "        ofile.write(json.dumps(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author paper counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000000 lines...\n",
      "processed 20000000 lines...\n",
      "processed 30000000 lines...\n",
      "processed 40000000 lines...\n",
      "processed 50000000 lines...\n",
      "processed 60000000 lines...\n",
      "processed 70000000 lines...\n",
      "processed 80000000 lines...\n",
      "processed 90000000 lines...\n",
      "processed 100000000 lines...\n",
      "processed 110000000 lines...\n",
      "processed 120000000 lines...\n",
      "processed 130000000 lines...\n",
      "processed 140000000 lines...\n",
      "processed 150000000 lines...\n",
      "processed 160000000 lines...\n",
      "processed 170000000 lines...\n",
      "processed 180000000 lines...\n",
      "processed 190000000 lines...\n",
      "processed 200000000 lines...\n",
      "processed 210000000 lines...\n",
      "processed 220000000 lines...\n"
     ]
    }
   ],
   "source": [
    "pid_year = {}\n",
    "\n",
    "for line in yield_one_line(mag_dir+'Papers.csv', delimiter=',', quoting=csv.QUOTE_ALL):\n",
    "    pid, year = line[0], line[7]\n",
    "    # lowercase all DOIs\n",
    "    # doi = doi.lower()\n",
    "    if year != '' and int(year) <= 2018:\n",
    "        pid_year[pid] = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219618489"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pid_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000000 lines...\n",
      "processed 20000000 lines...\n",
      "processed 30000000 lines...\n",
      "processed 40000000 lines...\n",
      "processed 50000000 lines...\n",
      "processed 60000000 lines...\n",
      "processed 70000000 lines...\n",
      "processed 80000000 lines...\n",
      "processed 90000000 lines...\n",
      "processed 100000000 lines...\n",
      "processed 110000000 lines...\n",
      "processed 150000000 lines...\n",
      "processed 160000000 lines...\n",
      "processed 170000000 lines...\n",
      "processed 180000000 lines...\n",
      "processed 190000000 lines...\n",
      "processed 200000000 lines...\n",
      "processed 210000000 lines...\n",
      "processed 220000000 lines...\n",
      "processed 230000000 lines...\n",
      "processed 240000000 lines...\n",
      "processed 250000000 lines...\n",
      "processed 260000000 lines...\n",
      "processed 270000000 lines...\n",
      "processed 280000000 lines...\n",
      "processed 290000000 lines...\n",
      "processed 300000000 lines...\n",
      "processed 310000000 lines...\n",
      "processed 320000000 lines...\n",
      "processed 330000000 lines...\n",
      "processed 340000000 lines...\n",
      "processed 350000000 lines...\n",
      "processed 360000000 lines...\n",
      "processed 370000000 lines...\n",
      "processed 380000000 lines...\n",
      "processed 390000000 lines...\n",
      "processed 400000000 lines...\n",
      "processed 410000000 lines...\n",
      "processed 420000000 lines...\n",
      "processed 430000000 lines...\n",
      "processed 440000000 lines...\n",
      "processed 450000000 lines...\n",
      "processed 460000000 lines...\n",
      "processed 470000000 lines...\n",
      "processed 480000000 lines...\n",
      "processed 490000000 lines...\n",
      "processed 500000000 lines...\n",
      "processed 510000000 lines...\n",
      "processed 520000000 lines...\n",
      "processed 530000000 lines...\n",
      "processed 540000000 lines...\n",
      "processed 550000000 lines...\n",
      "processed 560000000 lines...\n",
      "processed 570000000 lines...\n",
      "processed 580000000 lines...\n",
      "processed 590000000 lines...\n"
     ]
    }
   ],
   "source": [
    "aid_year_paper_count = defaultdict(lambda: defaultdict(int))\n",
    "aid_pids_upto_2017 = defaultdict(set)\n",
    "\n",
    "for line in yield_one_line(mag_dir+'PaperAuthorAffiliations.txt', delimiter='\\t', quoting=csv.QUOTE_NONE):\n",
    "    pid, aid = line[:2]\n",
    "    if pid in pid_year:\n",
    "        year = pid_year[pid]\n",
    "        if aid in all_aids:\n",
    "            aid_year_paper_count[aid][year] += 1\n",
    "            # only count citations to these authors' papers published up to 2017.\n",
    "            if int(year) <= 2017:\n",
    "                aid_pids_upto_2017[aid].add(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3334118"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aid_year_paper_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this counts pubs in all years.\n",
    "with open(fpath('aid_year_paper_count.json'), 'w') as ofile:\n",
    "    for aid, info in aid_year_paper_count.items():\n",
    "        row = {'aid': aid, 'paper_cn': info}\n",
    "        ofile.write(json.dumps(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2312690"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# many authors have 0 pubs before 2018 (their first pub was in 2018)\n",
    "len(aid_pids_upto_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_track = set()\n",
    "for aid in aid_pids_upto_2017:\n",
    "    for pid in aid_pids_upto_2017[aid]:\n",
    "        pids_track.add(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37436167"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pids_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000000 lines...\n",
      "processed 20000000 lines...\n",
      "processed 30000000 lines...\n",
      "processed 40000000 lines...\n",
      "processed 50000000 lines...\n",
      "processed 60000000 lines...\n",
      "processed 70000000 lines...\n",
      "processed 80000000 lines...\n",
      "processed 90000000 lines...\n",
      "processed 100000000 lines...\n",
      "processed 110000000 lines...\n",
      "processed 120000000 lines...\n",
      "processed 130000000 lines...\n",
      "processed 140000000 lines...\n",
      "processed 150000000 lines...\n",
      "processed 160000000 lines...\n",
      "processed 170000000 lines...\n",
      "processed 180000000 lines...\n",
      "processed 190000000 lines...\n",
      "processed 200000000 lines...\n",
      "processed 210000000 lines...\n",
      "processed 220000000 lines...\n",
      "processed 230000000 lines...\n",
      "processed 240000000 lines...\n",
      "processed 250000000 lines...\n",
      "processed 260000000 lines...\n",
      "processed 270000000 lines...\n",
      "processed 280000000 lines...\n",
      "processed 290000000 lines...\n",
      "processed 300000000 lines...\n",
      "processed 310000000 lines...\n",
      "processed 320000000 lines...\n",
      "processed 330000000 lines...\n",
      "processed 340000000 lines...\n",
      "processed 350000000 lines...\n",
      "processed 360000000 lines...\n",
      "processed 370000000 lines...\n",
      "processed 380000000 lines...\n",
      "processed 390000000 lines...\n",
      "processed 400000000 lines...\n",
      "processed 410000000 lines...\n",
      "processed 420000000 lines...\n",
      "processed 430000000 lines...\n",
      "processed 440000000 lines...\n",
      "processed 450000000 lines...\n",
      "processed 460000000 lines...\n",
      "processed 470000000 lines...\n",
      "processed 480000000 lines...\n",
      "processed 490000000 lines...\n",
      "processed 500000000 lines...\n",
      "processed 510000000 lines...\n",
      "processed 520000000 lines...\n",
      "processed 530000000 lines...\n",
      "processed 540000000 lines...\n",
      "processed 550000000 lines...\n",
      "processed 560000000 lines...\n",
      "processed 570000000 lines...\n",
      "processed 580000000 lines...\n",
      "processed 590000000 lines...\n",
      "processed 600000000 lines...\n",
      "processed 610000000 lines...\n",
      "processed 620000000 lines...\n",
      "processed 630000000 lines...\n",
      "processed 640000000 lines...\n",
      "processed 650000000 lines...\n",
      "processed 660000000 lines...\n",
      "processed 670000000 lines...\n",
      "processed 680000000 lines...\n",
      "processed 690000000 lines...\n",
      "processed 700000000 lines...\n",
      "processed 710000000 lines...\n",
      "processed 720000000 lines...\n",
      "processed 730000000 lines...\n",
      "processed 740000000 lines...\n",
      "processed 750000000 lines...\n",
      "processed 760000000 lines...\n",
      "processed 770000000 lines...\n",
      "processed 780000000 lines...\n",
      "processed 790000000 lines...\n",
      "processed 800000000 lines...\n",
      "processed 810000000 lines...\n",
      "processed 820000000 lines...\n",
      "processed 830000000 lines...\n",
      "processed 840000000 lines...\n",
      "processed 850000000 lines...\n",
      "processed 860000000 lines...\n",
      "processed 870000000 lines...\n",
      "processed 880000000 lines...\n",
      "processed 890000000 lines...\n",
      "processed 900000000 lines...\n",
      "processed 910000000 lines...\n",
      "processed 920000000 lines...\n",
      "processed 930000000 lines...\n",
      "processed 940000000 lines...\n",
      "processed 950000000 lines...\n",
      "processed 960000000 lines...\n",
      "processed 970000000 lines...\n",
      "processed 980000000 lines...\n",
      "processed 990000000 lines...\n",
      "processed 1000000000 lines...\n",
      "processed 1010000000 lines...\n",
      "processed 1020000000 lines...\n",
      "processed 1030000000 lines...\n",
      "processed 1040000000 lines...\n",
      "processed 1050000000 lines...\n",
      "processed 1060000000 lines...\n",
      "processed 1070000000 lines...\n",
      "processed 1080000000 lines...\n",
      "processed 1090000000 lines...\n",
      "processed 1100000000 lines...\n",
      "processed 1110000000 lines...\n",
      "processed 1120000000 lines...\n",
      "processed 1130000000 lines...\n",
      "processed 1140000000 lines...\n",
      "processed 1150000000 lines...\n",
      "processed 1160000000 lines...\n",
      "processed 1170000000 lines...\n",
      "processed 1180000000 lines...\n",
      "processed 1190000000 lines...\n",
      "processed 1200000000 lines...\n",
      "processed 1210000000 lines...\n",
      "processed 1220000000 lines...\n",
      "processed 1230000000 lines...\n",
      "processed 1240000000 lines...\n",
      "processed 1250000000 lines...\n",
      "processed 1260000000 lines...\n",
      "processed 1270000000 lines...\n",
      "processed 1280000000 lines...\n",
      "processed 1290000000 lines...\n",
      "processed 1300000000 lines...\n",
      "processed 1310000000 lines...\n",
      "processed 1320000000 lines...\n",
      "processed 1330000000 lines...\n",
      "processed 1340000000 lines...\n",
      "processed 1350000000 lines...\n",
      "processed 1360000000 lines...\n",
      "processed 1370000000 lines...\n",
      "processed 1380000000 lines...\n",
      "processed 1390000000 lines...\n",
      "processed 1400000000 lines...\n",
      "processed 1410000000 lines...\n",
      "processed 1420000000 lines...\n",
      "processed 1430000000 lines...\n",
      "processed 1440000000 lines...\n",
      "processed 1450000000 lines...\n",
      "processed 1460000000 lines...\n",
      "processed 1470000000 lines...\n"
     ]
    }
   ],
   "source": [
    "pid_cites_upto_2017 = defaultdict(int)\n",
    "\n",
    "for line in yield_one_line(mag_dir+'PaperReferences.txt', delimiter='\\t', quoting=csv.QUOTE_NONE):\n",
    "    pid, rid = line[0], line[1]\n",
    "    if pid in pid_year:\n",
    "        year = pid_year[pid]\n",
    "        # citing paper has to be published before 2018.\n",
    "        if int(year) <= 2017 and rid in pids_track:\n",
    "            pid_cites_upto_2017[rid] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24255365"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some papers may have 0 citations, so they won't be in this dictionary\n",
    "len(pid_cites_upto_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid_cites_upto_2017 = {}\n",
    "\n",
    "for aid in all_aids:\n",
    "    aid_cites_upto_2017[aid] = 0\n",
    "    if aid in aid_pids_upto_2017:\n",
    "        for pid in aid_pids_upto_2017[aid]:\n",
    "            aid_cites_upto_2017[aid] += pid_cites_upto_2017[pid]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2312690"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aid_cites_upto_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3335676"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aid_cites_upto_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fpath('aid_cites_upto_2017.json'), 'w') as ofile:\n",
    "    for aid, cn in aid_cites_upto_2017.items():\n",
    "        row = {'aid': aid, 'cites': cn}\n",
    "        ofile.write(json.dumps(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code not used below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000000 lines...\n",
      "processed 20000000 lines...\n",
      "processed 30000000 lines...\n",
      "processed 40000000 lines...\n",
      "processed 50000000 lines...\n",
      "processed 60000000 lines...\n",
      "processed 70000000 lines...\n"
     ]
    }
   ],
   "source": [
    "dois_abs = {}\n",
    "\n",
    "for line in yield_one_line(mag_dir+'PaperAbstractsInvertedIndex.txt', delimiter='\\t', quoting=csv.QUOTE_NONE):\n",
    "    pid, abs_dict = line\n",
    "    try:\n",
    "        abs_dict = json.loads(abs_dict)\n",
    "    except:\n",
    "        continue\n",
    "    if pid in pid_to_doi:\n",
    "        doi = pid_to_doi[pid]\n",
    "        length = abs_dict[\"IndexLength\"]\n",
    "        text = [''] * length\n",
    "        for word, ixs in abs_dict['InvertedIndex'].items():\n",
    "            for ix in ixs:\n",
    "                text[ix] = word\n",
    "        dois_abs[doi] = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156679"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dois_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fpath('dois_abstract.json'), 'w') as ofile:\n",
    "    for doi, text in dois_abs.items():\n",
    "        row = {'doi': doi, 'abs': text}\n",
    "        ofile.write(json.dumps(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAG keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dois_disc = defaultdict(list)\n",
    "\n",
    "for line in yield_one_line(mag_dir+'PaperFieldsOfStudy.txt', delimiter='\\t', quoting=csv.QUOTE_NONE):\n",
    "    pid, fid, frac = line\n",
    "    frac = float(frac)\n",
    "    if pid in pid_to_doi:\n",
    "        doi = pid_to_doi[pid]\n",
    "        dois_disc[doi].append((fid, frac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268867"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dois_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2780320433', 0.526666343),\n",
       " ('2779668308', 0.6762392),\n",
       " ('2780221984', 0.5918762),\n",
       " ('511355011', 0.544381559),\n",
       " ('2777391703', 0.6243656),\n",
       " ('555293320', 0.591008365),\n",
       " ('2910068830', 0.600785553),\n",
       " ('141071460', 0.387458026),\n",
       " ('2777180221', 0.6201425),\n",
       " ('71924100', 0.410550684)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dois_disc['10.1001/2012.jama.11132']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fpath('dois_fields_mag.json'), 'w') as ofile:\n",
    "    for doi in dois_disc:\n",
    "        fields = dois_disc[doi]\n",
    "        row = {'doi': doi, 'fields': fields}\n",
    "        ofile.write(json.dumps(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
